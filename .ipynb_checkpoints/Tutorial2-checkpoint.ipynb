{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ebf417-8a73-4888-a78d-f18f08ca5e21",
   "metadata": {},
   "source": [
    "## Tutorial 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2e583-a455-4610-b88f-36f6ad84571e",
   "metadata": {},
   "source": [
    "1. Open Your Terminal or Command Prompt\n",
    "2. Activate Your Conda Environment\n",
    "   conda activate MLLab\n",
    "3. Install Matplotlib\n",
    "   conda install matplotlib\n",
    "4. Verify Installation\n",
    "5.  import matplotlib.pyplot as plt\n",
    "6.  \n",
    "    print(plt.matplotlib.__version__)\n",
    "\n",
    "After installation, it's a good practice to verify that `matplotlib` has been successfully installed. \n",
    "\n",
    "This line of code accesses the matplotlib package through the plt object and then prints the version of matplotlib using the __version__ attribute. \n",
    "This command imports `matplotlib` and prints its version, confirming the installation was successful.\n",
    "\n",
    "7. Install statsmodel using:\n",
    " pip install statsmodels\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86b9fd-5683-4d6c-ae45-aaa00fb52e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "print(plt.matplotlib.__version__)\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38b361-d627-4fbc-9f16-ec4709ef5d98",
   "metadata": {},
   "source": [
    "Pandas is built on the Numpy package and its key data structure is called the DataFrame. \n",
    "DataFrames allow you to store and manipulate tabular data in rows of observations and columns of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9382e32-ab59-4d11-9d8c-12d4732f4e51",
   "metadata": {},
   "source": [
    "Let's create a DataFrame from a dictionary. Here, each key becomes a column in the DataFrame, and the values are the data entries for those columns. A dictionary is a built-in data type that stores collections of data as key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b3faf-9364-4e4d-8640-c209800b1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 40, 45],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ed5e4-c4ec-4507-8709-c0659a25aa01",
   "metadata": {},
   "source": [
    "display the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef28a25-b5ee-4808-88b1-31ce6419b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26189145-a3bd-46d7-8b89-635baedba772",
   "metadata": {},
   "source": [
    "#### Select Data\n",
    "Select 'Name' and 'Age' Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed7c60-561c-4a86-92f0-97f1edbfefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Name', 'Age']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00510348-6f61-461a-9117-d2639e4fff21",
   "metadata": {},
   "source": [
    "#### Compute Basic Statistics\n",
    "Compute basic statistics for the numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf74f50-4a92-452a-b595-65570ef31aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2badbcfb-31c7-45b6-b467-304739319b74",
   "metadata": {},
   "source": [
    "#### Query the DataFrame\n",
    "Query the DataFrame for people older than 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be273ccf-5b4e-4320-93fe-324425438e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Age'] > 30]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd71ce4-923a-42f3-bd1a-85b5673fe474",
   "metadata": {},
   "source": [
    "This example demonstrates the ease of data manipulation with pandas, including creating a DataFrame, selecting specific columns, computing statistics, and filtering data based on conditions. Pandas is an extensive library, supporting a wide range of data manipulation and analysis tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c2d42-13e6-4c56-9e21-8791393d921a",
   "metadata": {},
   "source": [
    "#### Communities and Crime Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097a948-fc26-4980-b2e1-26b6632ff21f",
   "metadata": {},
   "source": [
    "The dataset used is Communities and Crime data from https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime. The attribute to be predicted is (Per Capita Violent Crimes). The variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b58592-d8b4-4f41-bd74-55af6a02fec4",
   "metadata": {},
   "source": [
    "#### attributes.csv contains the column names/names of variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd638f48-e204-4e30-92c2-da84ab0c5556",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m attrib \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/sitani/Documents/HertieSchool/Tutorials/communitiesandcrime/attributes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delim_whitespace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "attrib = pd.read_csv('/Users/sitani/Documents/HertieSchool/Tutorials/communitiesandcrime/attributes.csv', delim_whitespace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ef1f0-ee5a-4488-a194-25c4bdac7380",
   "metadata": {},
   "source": [
    "delim_whitespace=True is effectively splitting each line into separate columns based on whitespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff6f73-6848-4cd3-8b8f-bf1d4bcd88e5",
   "metadata": {},
   "source": [
    "Read the communities.data CSV file into a pandas DataFrame and use attrib Dataframe to define the column names of this new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514f93c-01e8-4c96-8827-fa276d5f5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/sitani/Documents/HertieSchool/Tutorials/communitiesandcrime/communities.data', names = attrib['attributes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69652c2-2844-490d-8d3f-df4119f736f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84420f37-8b0f-42b4-8c58-0903861af61c",
   "metadata": {},
   "source": [
    "This data has 1994 samples and 128 features/variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782aad4-d9c5-440e-9880-d756c900b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbee48-c202-41bc-b65a-7f34beb55c71",
   "metadata": {},
   "source": [
    "#### Remove non-predictive features\n",
    "\n",
    "1. state: US state (by number) - not counted as predictive above, but if considered, should be considered nominal (nominal)\n",
    "\n",
    "2. county: numeric code for county - not predictive, and many missing values (numeric)\n",
    "\n",
    "3. community: numeric code for community - not predictive and many missing values (numeric)\n",
    "\n",
    "4. communityname: community name - not predictive - for information only (string)\n",
    "\n",
    "5. fold: fold number for non-random 10 fold cross validation, potentially useful for debugging, paired tests - not predictive (numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d4038-f5cb-418b-8be2-cbab0ec476ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['state','county',\n",
    "                          'community','communityname',\n",
    "                          'fold'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb518b-7519-4f5c-ab2d-edb521b1d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb0cc5-7f4d-48d9-b071-42889ae147ee",
   "metadata": {},
   "source": [
    "Now the data has 123 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09091b6-393a-4abf-99c6-ad2a2410e302",
   "metadata": {},
   "source": [
    "#### Checking for Missing Data\n",
    "Marking Missing values in the dataset from ? to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea6de3-e176-4baa-808a-9ba7260d1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('?', np.nan)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536922f-e411-494d-9dae-6afaea3be735",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_miss = data.columns[data.isnull().any()]\n",
    "\n",
    "print(feat_miss)\n",
    "feat_miss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239476a-0032-4e7a-aa81-3c8882460944",
   "metadata": {},
   "source": [
    "##### From 122 predictive features, 23 contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42d1ca-518c-4089-9d68-2767ab61bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the features with missing values\n",
    "\n",
    "data[feat_miss[0:13]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f796d1-c271-4e8b-baa8-9dd1d9c25380",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[feat_miss[13:23]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0ad26-f9ca-4f9b-a8af-939499047567",
   "metadata": {},
   "source": [
    "#### Only OtherperCap has 1 missing value, rest have a lot of missing values.\n",
    "The missing value in feature OtherPerCap will be filled by a mean value using SimpleImputer class from sklearn.\n",
    "\n",
    "The others features present many missing values, and just for simplicity’s sake, we will remove them from the data set.\n",
    "\n",
    "The SimpleImputer class provides basic strategies for imputing (filling in) missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n",
    "\n",
    "For more info: https://scikit-learn.org/stable/modules/impute.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b988f25-b08c-4080-be8b-e3c676c01f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e3df2-9ab2-4a6f-ae91-412af0b42ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of SimpleImputer with mean strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Fit the imputer on the data and transform the column in one step\n",
    "data['OtherPerCap'] = imputer.fit_transform(data[['OtherPerCap']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7444b48-1087-4335-bccc-a26bdbab967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=1)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593b68c-92b2-4141-b26b-7dd7565f0454",
   "metadata": {},
   "source": [
    "Now, the data has 101 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94523934-4628-43d4-ab0a-2b568eaec3e8",
   "metadata": {},
   "source": [
    "### Summary Statistics of a Dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3c693-8ec8-4687-b239-be4e3ce63094",
   "metadata": {},
   "source": [
    "count: The number of non-missing (non-NaN) values.\n",
    "\n",
    "mean: The mean of the values.\n",
    "\n",
    "std: The standard deviation of the values.\n",
    "\n",
    "min: The minimum value.\n",
    "\n",
    "25%: The 25th percentile (first quartile).\n",
    "\n",
    "50% (median): The median of the data.\n",
    "\n",
    "75%: The 75th percentile (third quartile).\n",
    "\n",
    "max: The maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a500341-5a70-4d2d-a831-5e6927759664",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea388b86-ee97-4e17-a68b-9462c480a2d2",
   "metadata": {},
   "source": [
    " To specify percentiles other than the default values 25, 50, 75.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ad2bb-1187-453c-8f2d-e3fdc3925203",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_percentiles = data.describe(percentiles=[.20, .40, .60, .80])\n",
    "print(custom_percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6618374-921d-453f-896e-1bf4bfadeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# https://seaborn.pydata.org/\n",
    "# https://matplotlib.org/\n",
    "# ViolentCrimesPerPop is the output variable in the dataset\n",
    "plt.hist(data['ViolentCrimesPerPop'], bins=30, alpha=0.7, color='red')\n",
    "plt.title('Distribution of Per Capita Violent Crimes')\n",
    "plt.xlabel('Violent Crimes per 100,000 People')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b09ac-161a-4087-9211-95b50cc53293",
   "metadata": {},
   "source": [
    "### Splitting the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c71491-8c2c-4d00-8356-cc0d7a058b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:100].values #(data)\n",
    "y = data.iloc[:, 100].values  #(the attribute/feature to be predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99b34a-2d64-4bd1-86ec-4b32f865a476",
   "metadata": {},
   "source": [
    "The .iloc attribute in pandas is a powerful indexing method used for integer-location based indexing.\n",
    ".iloc is part of pandas' and provides a way to access a subset of the data frame's rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a08c8-af3b-4f6a-9cca-e0a537191973",
   "metadata": {},
   "source": [
    "1. Importing train_test_split Function: The **from sklearn.model_selection import train_test_split** command imports the train_test_split function from scikit-learn,    which is used to split the dataset into training and test sets.\n",
    "\n",
    "2. Setting the Random Seed: **seed = 0** sets the seed for the random number generator to 0. This ensures that the results are reproducible; anyone running this code with the same dataset and seed will get the same split of data.\n",
    "\n",
    "3. Splitting the Dataset: **X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = seed)** splits the features (X) and the target variable (y) into training and test sets. 30% (test_size = 0.3) of the data is allocated to the test set, while the remaining 70% is used for training the model.\n",
    "The random_state = seed parameter ensures that the split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb40371-c983-495f-a59d-15bed2faee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffed90c-235d-4db9-98b1-a55fba6f8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = seed)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598586e5-fe9b-400c-a201-7a39ceb45723",
   "metadata": {},
   "source": [
    "### Standardization:\n",
    "Standardization refers to the process of transforming each feature in your data so that it has a mean of 0 and a standard deviation of 1. This is done by subtracting the mean of each feature and then dividing by the standard deviation for each feature. The formula used is:\n",
    "\n",
    "z=(x-μ)/σ\n",
    "Here, x is the original feature value\n",
    "μ is the mean of the feature, and \n",
    "σ is the standard deviation of the feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072f6e9-b2f5-4fc8-99c9-94ae5b72591e",
   "metadata": {},
   "source": [
    "1. Importing StandardScaler:**from sklearn.preprocessing import StandardScaler** imports the StandardScaler class, which provides the functionality to standardize features.\n",
    "\n",
    "2. Creating a StandardScaler Instance: **sc = StandardScaler()** creates an instance of StandardScaler. This instance will be used to compute the mean and standard deviation for each feature in the dataset.\n",
    "   \n",
    "3. Fitting and Transforming the Training Data: **X_train = sc.fit_transform(X_train)** computes the mean and standard deviation of each feature in the training set X_train, and then standardizes the training set by applying the transformation z=(x-μ)/σ.\n",
    "\n",
    "  ​The fit_transform method is a combination of fit (to compute the scaling parameters) and transform (to apply the standardization).    The standardized training data is then reassigned to X_train.\n",
    "\n",
    "4. Transforming the Testing Data: **X_test = sc.transform(X_test)** applies the same transformation to X_test using the mean and standard deviation calculated from the training set. It's crucial to use the parameters from the training set to ensure the model evaluates on the same scale. The standardized test data is reassigned to X_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e815b0-c432-49a4-8023-78be47f094a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee41ce-4efa-4393-8407-be24383e33cc",
   "metadata": {},
   "source": [
    "Calculate Coefficients of Ordinary Least Squares Regression:\n",
    "1. Define a function to estimate the coefficients performing the matrix operations for the closed form solution. \n",
    "2. using stasmodels\n",
    "3. Compare the coefficients calculated from the two methods\n",
    "4. also calculate the predictions from the two methods and find nmse between the actual value of the output variable and your predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc0b44-1acb-47b7-b366-41b260fb3d48",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Regression using the closed form estimate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583602f3-d888-4f14-8a9a-ade4eb2cac48",
   "metadata": {},
   "source": [
    "Let's define a function to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a769a-7bdf-414d-987e-71ac631a4c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_closed_form(X_train, y_train, X_test=None):\n",
    "    \"\"\"\n",
    "    Performs linear regression using the closed-form solution.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training features, numpy array of shape (n_samples, n_features)\n",
    "    - y_train: Training target, numpy array of shape (n_samples,)\n",
    "    - X_test: Optional, test features, numpy array of shape (n_samples_test, n_features)\n",
    "\n",
    "    Returns:\n",
    "    - beta: Coefficients estimated from the training data\n",
    "    - predictions: Optional, predictions made on the test data if X_test is provided\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Add intercept term to training and optionally to test data\n",
    "    X_train_with_intercept = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "    if X_test is not None:\n",
    "        X_test_with_intercept = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "    \n",
    "    # Calculate coefficients using the closed-form solution\n",
    "    beta = np.linalg.inv(X_train_with_intercept.T @ X_train_with_intercept) @ X_train_with_intercept.T @ y_train # \n",
    "    \n",
    "    # Make predictions on the test set if provided\n",
    "    predictions = None\n",
    "    if X_test is not None:\n",
    "        predictions = X_test_with_intercept @ beta\n",
    "    \n",
    "    return beta, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c375b1-d0a2-4016-9a39-cc72f9156dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta, predictions_closedform = linear_regression_closed_form(X_train, y_train, X_test)\n",
    "\n",
    "print(\"Estimated coefficients:\", beta)\n",
    "print(beta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c9b9c-6125-4bb0-a041-45a8fe8f5dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predictions_closedform is not None:\n",
    "    print(\"Predictions on test set using closed form solution:\", predictions_closedform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d12dce-bd8f-4fdc-a423-52e926ae21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add intercept term for closed-form solution\n",
    "# X_train_with_intercept = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "# X_test_with_intercept = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585797f-a53e-4646-8641-b4e206a6e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Closed-form solution\n",
    "# beta = np.linalg.inv(X_train_with_intercept.T @ X_train_with_intercept) @ X_train_with_intercept.T @ y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a09b2-f72d-40bb-8e71-61774735ce2c",
   "metadata": {},
   "source": [
    "### Calculate Coefficients using Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a686917-370c-4cb3-a8f8-e75dbdf8633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e3416-96da-4a16-8750-f8c15fbb8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term for statsmodels\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y_train, X_train_sm).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f61ed9-280a-4f9d-b7ef-e3681d859b9a",
   "metadata": {},
   "source": [
    "Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fcf85-0680-4e1f-b061-6b8c426f19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using statsmodels\n",
    "predictions_statsmodels = model.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1502ae7-f035-4f0f-84a4-60faa8b2ce04",
   "metadata": {},
   "source": [
    "Calculate MSE for Both Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2da20a-b4ad-44e9-ad4e-42f6068bea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_numpy = mean_squared_error(y_test, predictions_closedform)\n",
    "mse_statsmodels = mean_squared_error(y_test, predictions_statsmodels)\n",
    "\n",
    "print(\"MSE for closed-form solution:\", mse_numpy)\n",
    "print(\"MSE for statsmodels:\", mse_statsmodels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26436d0-8b8a-4d79-8093-d8ca4e400a7a",
   "metadata": {},
   "source": [
    "Compare the Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f630f65-a423-4838-94ef-081ca05abdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients from closed-form solution:\", beta)\n",
    "print(\"Coefficients from statsmodels:\", model.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02499c1-272c-4c8b-882b-c54abd441afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  beta is from the closed-form solution and coefficients_sm is from statsmodels\n",
    "difference = beta - model.params  # Make sure both are numpy arrays for direct subtraction\n",
    "print(\"Coefficient Differences:\", difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5a249-fcf0-4580-8f5c-7de6a1acd998",
   "metadata": {},
   "source": [
    "### Visual Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55776c-e1eb-452a-a269-f98a4d69a384",
   "metadata": {},
   "source": [
    " Line Plot: For a comparison of how each coefficient from the two models aligns, a line plot can be effective.\n",
    "\n",
    " plt: we imported matplotlib.pyplot as plt in the beginning. Remember?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96748a-bb1b-4a12-97d6-b1d40065ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(beta, label='Closed-Form', marker='o')\n",
    "plt.plot(model.params, label='Statsmodels', marker='x')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.title('Comparison of Coefficients')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b127b-1097-40a4-8d11-f624ac0ae69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
